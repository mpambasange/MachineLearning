{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mpambasange/MachineLearning/blob/master/bag_of_words.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzM-VytVcaJD"
      },
      "source": [
        "# Image Recognition Using Bags of Words"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zhj7alfURBA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JU7AlNYqce9f"
      },
      "source": [
        "### Referenced Code \n",
        "\n",
        "BoW :https://github.com/CyrusChiu/Image-recognition\n",
        "\n",
        "K-Means Clustering using GPU : https://github.com/ilyaraz/pytorch_kmeans\n",
        "\n",
        "Multi-class Linear SVM using GPU : https://github.com/murtazajafferji/svm-gpu\n",
        "\n",
        "\n",
        "Porting by glee1228@naver.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvWidhwG3nhN"
      },
      "source": [
        "## Google Drive Link\n",
        "for Downloading pkl files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ScN5ZGV3mMf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "62dba396-5b91-4566-e8d5-fd1bac0ba3de"
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "if os.path.exists('/content/gdrive')==False:\n",
        "    drive.mount('/content/gdrive')\n",
        "    print('Google Drive is mounted\\n')\n",
        "else:\n",
        "    print('Google Drive is already mounted\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "Google Drive is mounted\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qavf8WwP5_Uo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "ccf0142d-2e39-449c-e56d-d023dedf8cc0"
      },
      "source": [
        "os.chdir('/content/gdrive/My Drive')\n",
        "print('current path ? ',os.getcwd())\n",
        "print('List of files in the current path :',os.listdir(os.getcwd()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "current path ?  /content/gdrive/My Drive\n",
            "List of files in the current path : ['101_ObjectCategories.tar.gz.1', '101_ObjectCategories.tar.gz.2', '이동훈_입과증빙서류.zip', '1018_SQL 전문가 가이드 2010 Edition.pdf', '123.mp4', '공유폴더', '교안및보고서', 'Colab Notebooks', 'foo.txt', 'workspace', 'sample_cardiac', 'retrain.py', '3DUnetCNN-master', 'segmentation', 'generic_Unet', '참가동의서.pdf', 'train.csv', 'VLAD.ipynb', 'LeNet5_Practice.ipynb', 'resized_Dataset', 'query', 'Sejongbuild_images', 'oxbuild_images', 'AILeader_Dataset', 'input', 'output', '파이썬기계학습', 'deepfashion2', '머신러닝 인터뷰 준비.gdoc', '101_ObjectCategories', 'DatasetFile.txt', 'LabelFile.txt', 'x_train_img.pkl', 'x_test_img.pkl', 'y_train_img.pkl', 'y_test_img.pkl', 'y_train.pkl', 'y_test.pkl', 'x_test.pkl', 'x_train.pkl', 'bow_codebook.pkl', 'x_train_hist.pkl', 'x_test_hist.pkl', 'test_data', '6.Chatbot(QA-seq2seq)-Word_onehot.ipynb', '6.Chatbot(QA-seq2seq with Attention).ipynb', '7.Chatbot(QA-seq2seq).ipynb', 'nlp-tutorial-master', '백업', 'cs231n']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZYr9prraqbZ"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn import svm\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import numpy as np\n",
        "import random\n",
        "import sklearn\n",
        "import scipy.cluster.vq as vq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZrl5OMr7I-E"
      },
      "source": [
        "## Bag of Features Process\n",
        "\n",
        "### 1. Prepare Dataset\n",
        "Caltech101 , Cifar10 ... etc\n",
        "### 2. feature extraction\n",
        "SIFT descriptors , Dense SIFT descriptors ... etc\n",
        "### 3. clustering and build codebook\n",
        "K-means clustering algorithm .... etc\n",
        "### 4. Image representation(making the histogram of features)\n",
        "Vector Quantization Technique\n",
        "### 5. classifier learning and recognition\n",
        "SVM , Naive Bayes, ...etc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq6X9t2OSlE9"
      },
      "source": [
        "### run_all_process : True\n",
        "\n",
        "End-To-End Learning and Recognition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSCIr0geSivG"
      },
      "source": [
        "run_all_process = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WovZX982y0_Q"
      },
      "source": [
        "VOC_SIZE = 200              # the number of cluster center point\n",
        "NUMBER_IMAGES_CLASS = 35      # the number of images per class\n",
        "SIFT_type = 'sparse'           # The type of SIFT feature  : dense or sparse\n",
        "\n",
        "# C_range = 10.0 ** np.arange(-2, 4)\n",
        "# print(dict(C=C_range.tolist()))\n",
        "PARAM_GRID = dict(C=[1.0,10.0,50.0,70.0,100.0,150.0,200.0,250.0,300.0,500.0])   # Parameter C of SVM (linear kernel)\n",
        "random.seed(777)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zv4WK2FUwd-H"
      },
      "source": [
        "prepare_dataset = False\n",
        "feature_extraction = True\n",
        "clustering_and_build_codebook = True\n",
        "image_representation = True\n",
        "learning_and_recognition = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK61NIWaynce"
      },
      "source": [
        "if run_all_process:\n",
        "    prepare_dataset = True\n",
        "    feature_extraction = True\n",
        "    clustering_and_build_codebook = True\n",
        "    image_representation = True\n",
        "    learning_and_recognition = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEUbFJqTNFGM"
      },
      "source": [
        "## Prepare Caltech-101 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IED6KUByP2R0"
      },
      "source": [
        "def get_sample_datalist(path='./101_ObjectCategories/DatasetFile.txt', new_path=\"./101_ObjectCategories/SampleDatafile.txt\",count=30):\n",
        "    print('Getting sample data list..')\n",
        "    with open(path, 'r') as f:\n",
        "        paths = f.readlines()\n",
        "    count_dict = {}\n",
        "    new_x, new_y = [], []\n",
        "    for each in paths:\n",
        "        each = each.strip()\n",
        "        \n",
        "        label, path = each.split('$')\n",
        "        if not label in count_dict.keys():\n",
        "            count_dict[label] = 1\n",
        "            new_x.append(path)\n",
        "            new_y.append(label)\n",
        "        else:\n",
        "            if count_dict[label] >= count:\n",
        "                pass\n",
        "            else:\n",
        "                count_dict[label] += 1\n",
        "                new_x.append(path)\n",
        "                new_y.append(label)\n",
        "\n",
        "    result = zip(new_x, new_y)\n",
        "    f = open(new_path, 'w')\n",
        "    for x, y in result:\n",
        "        f.write('{}${}\\n'.format(y, x))\n",
        "    f.close()\n",
        "\n",
        "    return new_path,[new_x, new_y]\n",
        "\n",
        "def load_my_data(path, test=None):\n",
        "    print('load_my_data')\n",
        "    with open(path, 'r') as f:\n",
        "        paths = f.readlines()\n",
        "    x, y = [], []\n",
        "    for each in paths:\n",
        "        each = each.strip()\n",
        "        label, path = each.split('$')\n",
        "        img = cv2.imread(path)\n",
        "        if img.shape[:2] != (256,256):\n",
        "            img = cv2.resize(img, (256,256))\n",
        "        x.append(img)\n",
        "        y.append(label)\n",
        "    return [x, y]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksjkxURExDJI"
      },
      "source": [
        "## Opencv Downgrade\n",
        "3.4.3 version can not be used because of patents on SIFT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49HAzRqNwwF6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "5e095c28-3567-4623-deca-db22c8471e24"
      },
      "source": [
        "! yes | pip3 uninstall opencv-python\n",
        "\n",
        "! yes | pip3 uninstall opencv-contrib-python\n",
        "\n",
        "! yes | pip3 install opencv-python==3.4.2.16\n",
        " \n",
        "! yes | pip3 install opencv-contrib-python==3.4.2.16"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling opencv-python-4.1.2.30:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.6/dist-packages/cv2/*\n",
            "    /usr/local/lib/python3.6/dist-packages/opencv_python-4.1.2.30.dist-info/*\n",
            "Proceed (y/n)?   Successfully uninstalled opencv-python-4.1.2.30\n",
            "Uninstalling opencv-contrib-python-4.1.2.30:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.6/dist-packages/opencv_contrib_python-4.1.2.30.dist-info/*\n",
            "Proceed (y/n)?   Successfully uninstalled opencv-contrib-python-4.1.2.30\n",
            "Collecting opencv-python==3.4.2.16\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/7d/5042b668a8ed41d2a80b8c172f5efcd572e3c046c75ae029407e19b7fc68/opencv_python-3.4.2.16-cp36-cp36m-manylinux1_x86_64.whl (25.0MB)\n",
            "\u001b[K     |████████████████████████████████| 25.0MB 130kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from opencv-python==3.4.2.16) (1.18.5)\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: opencv-python\n",
            "Successfully installed opencv-python-3.4.2.16\n",
            "Collecting opencv-contrib-python==3.4.2.16\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/f1/66330f4042c4fb3b2d77a159db8e8916d9cdecc29bc8c1f56bc7f8a9bec9/opencv_contrib_python-3.4.2.16-cp36-cp36m-manylinux1_x86_64.whl (30.6MB)\n",
            "\u001b[K     |████████████████████████████████| 30.6MB 102kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from opencv-contrib-python==3.4.2.16) (1.18.5)\n",
            "Installing collected packages: opencv-contrib-python\n",
            "Successfully installed opencv-contrib-python-3.4.2.16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPMeXNyJ0uQx"
      },
      "source": [
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9oSfWpjNIuB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2e69d80f-700b-4d7d-a885-2eefac97938a"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "if prepare_dataset:\n",
        "  ! wget http://www.vision.caltech.edu/Image_Datasets/Caltech101/101_ObjectCategories.tar.gz\n",
        "  ! tar -xvzf 101_ObjectCategories.tar.gz\n",
        "  ! rm -rf 101_ObjectCategories.tar.gz\n",
        "  dataset_path = os.path.abspath('101_ObjectCategories')\n",
        "  Dataset_File = open(os.path.join(dataset_path,'DatasetFile.txt'), 'w')\n",
        "  label_file = open(os.path.join(dataset_path,'LabelFile.txt'),'w')\n",
        "  for class_idx, label in enumerate(os.scandir(dataset_path)):\n",
        "  #   print(class_idx,label.name)\n",
        "    label_file.write(str(class_idx)+'$'+label.name+'\\n')\n",
        "    image_paths = glob.glob(os.path.join(label.path,'*.jpg'))\n",
        "    length = len(image_paths)\n",
        "    for path in image_paths[:int(length)]:\n",
        "      Dataset_File.write(\"%d$\"% class_idx+path+\"\\n\")\n",
        "\n",
        "  Dataset_File.close()\n",
        "  label_file.close()\n",
        "  new_path,_=get_sample_datalist('./101_ObjectCategories/DatasetFile.txt','./101_ObjectCategories/SampleDatafile.txt',NUMBER_IMAGES_CLASS) # Data Sampling\n",
        "  x_train,y_train=load_my_data(new_path) # load Data\n",
        "  x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size = 0.1, random_state = 77) #Split Train / Test Dataset\n",
        "  \n",
        "  with open('./x_train_img.pkl','wb') as f:\n",
        "    pickle.dump(x_train, f)\n",
        "  with open('./x_test_img.pkl','wb') as f:\n",
        "    pickle.dump(x_test, f)\n",
        "  with open('./y_train_img.pkl', 'wb') as f:\n",
        "    pickle.dump(y_train, f)\n",
        "  with open('./y_test_img.pkl', 'wb') as f:\n",
        "    pickle.dump(y_test, f)\n",
        "    \n",
        "else:\n",
        "  with open('./x_train_img.pkl','rb') as fp:\n",
        "    x_train = pickle.load(fp)\n",
        "  with open('./x_test_img.pkl','rb') as fp:\n",
        "    x_test = pickle.load(fp)\n",
        "  with open('./y_train_img.pkl','rb') as fp:\n",
        "    y_train = pickle.load(fp)\n",
        "  with open('./y_test_img.pkl','rb') as fp:\n",
        "    y_test = pickle.load(fp)\n",
        "    \n",
        "  print(\"Caltech-101 Images ard loaded\")\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Caltech-101 Images ard loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFAmkwS-tfMd"
      },
      "source": [
        "import torch\n",
        "import random\n",
        "import sys\n",
        "\n",
        "device_gpu = torch.device('cuda')\n",
        "device_cpu = torch.device('cpu')\n",
        "\n",
        "# Choosing `num_centers` random data points as the initial centers\n",
        "def random_init(dataset, num_centers):\n",
        "    num_points = dataset.size(0)\n",
        "    dimension = dataset.size(1)\n",
        "    used = torch.zeros(num_points, dtype=torch.long)\n",
        "    indices = torch.zeros(num_centers, dtype=torch.long)\n",
        "    for i in range(num_centers):\n",
        "        while True:\n",
        "            cur_id = random.randint(0, num_points - 1)\n",
        "            if used[cur_id] > 0:\n",
        "                continue\n",
        "            used[cur_id] = 1\n",
        "            indices[i] = cur_id\n",
        "            break\n",
        "    indices = indices.to(device_gpu)\n",
        "    centers = torch.gather(dataset, 0, indices.view(-1, 1).expand(-1, dimension))\n",
        "    return centers\n",
        "\n",
        "# Compute for each data point the closest center\n",
        "def compute_codes(dataset, centers):\n",
        "    num_points = dataset.size(0)\n",
        "    dimension = dataset.size(1)\n",
        "    num_centers = centers.size(0)\n",
        "    # 5e8 should vary depending on the free memory on the GPU\n",
        "    # Ideally, automatically ;)\n",
        "    chunk_size = int(5e8 / num_centers)\n",
        "    codes = torch.zeros(num_points, dtype=torch.long, device=device_gpu)\n",
        "    centers_t = torch.transpose(centers, 0, 1)\n",
        "    centers_norms = torch.sum(centers ** 2, dim=1).view(1, -1)\n",
        "    for i in range(0, num_points, chunk_size):\n",
        "        begin = i\n",
        "        end = min(begin + chunk_size, num_points)\n",
        "        dataset_piece = dataset[begin:end, :]\n",
        "        dataset_norms = torch.sum(dataset_piece ** 2, dim=1).view(-1, 1)\n",
        "        distances = torch.mm(dataset_piece, centers_t)\n",
        "        distances *= -2.0\n",
        "        distances += dataset_norms\n",
        "        distances += centers_norms\n",
        "        _, min_ind = torch.min(distances, dim=1)\n",
        "        codes[begin:end] = min_ind\n",
        "    return codes\n",
        "\n",
        "# Compute new centers as means of the data points forming the clusters\n",
        "def update_centers(dataset, codes, num_centers):\n",
        "    num_points = dataset.size(0)\n",
        "    dimension = dataset.size(1)\n",
        "    centers = torch.zeros(num_centers, dimension, dtype=torch.float, device=device_gpu)\n",
        "    cnt = torch.zeros(num_centers, dtype=torch.float, device=device_gpu)\n",
        "    centers.scatter_add_(0, codes.view(-1, 1).expand(-1, dimension), dataset)\n",
        "    cnt.scatter_add_(0, codes, torch.ones(num_points, dtype=torch.float, device=device_gpu))\n",
        "    # Avoiding division by zero\n",
        "    # Not necessary if there are no duplicates among the data points\n",
        "    cnt = torch.where(cnt > 0.5, cnt, torch.ones(num_centers, dtype=torch.float, device=device_gpu))\n",
        "    centers /= cnt.view(-1, 1)\n",
        "    return centers\n",
        "\n",
        "def cluster(dataset, num_centers):\n",
        "    centers = random_init(dataset, num_centers)\n",
        "    codes = compute_codes(dataset, centers)\n",
        "    num_iterations = 0\n",
        "    while True:\n",
        "        sys.stdout.write('.')\n",
        "        sys.stdout.flush()\n",
        "        num_iterations += 1\n",
        "        centers = update_centers(dataset, codes, num_centers)\n",
        "        new_codes = compute_codes(dataset, centers)\n",
        "        # Waiting until the clustering stops updating altogether\n",
        "        # This is too strict in practice\n",
        "        if torch.equal(codes, new_codes):\n",
        "            sys.stdout.write('\\n')\n",
        "            print('Converged in %d iterations' % num_iterations)\n",
        "            break\n",
        "        codes = new_codes\n",
        "    return centers, codes\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p59LwB-cjgbj"
      },
      "source": [
        "def extract_sift_descriptors(img):\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    sift = cv2.xfeatures2d.SIFT_create()\n",
        "    keypoints, descriptors = sift.detectAndCompute(gray, None)\n",
        "    return descriptors\n",
        "\n",
        "def extract_DenseSift_descriptors(img):\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    sift = cv2.xfeatures2d.SIFT_create()\n",
        "    dsift_step_size = 4\n",
        "    kps = [cv2.KeyPoint(x,y,dsift_step_size)\n",
        "                for y in range(0, gray.shape[0],dsift_step_size)\n",
        "                for x in range(0, gray.shape[1],dsift_step_size)]\n",
        "    kp, des = sift.compute(gray, kps)\n",
        "    \n",
        "    return [kp, des]\n",
        "\n",
        "def build_codebook(X, voc_size):\n",
        "    \"\"\"\n",
        "    Inupt a list of feature descriptors\n",
        "    voc_size is the \"K\" in K-means, k is also called vocabulary size\n",
        "    Return the codebook/dictionary\n",
        "    \"\"\"\n",
        "    features = np.vstack((descriptor for descriptor in X)).astype(np.float32)\n",
        "    dataset = torch.from_numpy(features).to(torch.device('cuda'))\n",
        "    print('Starting clustering')\n",
        "    centers, codes = cluster(dataset, voc_size)\n",
        "    return centers.cpu()\n",
        "\n",
        "\n",
        "def input_vector_encoder(feature, codebook):\n",
        "    \"\"\"\n",
        "    Input all the local feature of the image\n",
        "    Pooling (encoding) by codebook and return\n",
        "    \"\"\"\n",
        "    code, _ = vq.vq(feature, codebook)\n",
        "    word_hist, bin_edges = np.histogram(code, bins=range(codebook.shape[0] + 1), normed=True)\n",
        "    return word_hist\n",
        "\n",
        "def bootstrap_x_y_resample(x,y, n=None):\n",
        "    if n == None:\n",
        "        n = len(x)\n",
        "    if len(x)!=len(y):\n",
        "        print('the number of data is not match the number of label')\n",
        "    resample_i = np.floor(np.random.rand(n) * len(x)).astype(int)\n",
        "    X_resample = np.array(x)[resample_i]\n",
        "    Y_resample = np.array(y)[resample_i]\n",
        "    # print('original label mean:', sum(np.array(y).astype(int)) / len(y))\n",
        "    # print('resampled label mean:', sum(np.array(Y_resample).astype(int)) / len(Y_resample))\n",
        "    return X_resample,Y_resample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dj6-oMokPFbL"
      },
      "source": [
        "def svm_classifier(x_train, y_train, x_test=None, y_test=None,param_grid={}):\n",
        "    if x_test is None and y_test is None:\n",
        "        x_train, x_test, y_train, y_test = train_test_split(\n",
        "                x_train, y_train, test_size=0.2, random_state=6)\n",
        "        print(\"Spliting train:{}/test:{} from training data\".format(len(x_train), len(x_test)))\n",
        "\n",
        "    #### Time to spend too much\n",
        "#     gamma_range = 10.0 ** np.arange(-4, 4)\n",
        "#     param_grid = dict(gamma=gamma_range.tolist(), C=C_range.tolist())\n",
        "    # Grid search for C, gamma, 5-fold CV\n",
        "    print(\"Tuning hyper-parameters\\n\")\n",
        "#     clf = GridSearchCV(svm.SVC(), param_grid, cv=5, n_jobs=-1)\n",
        "    clf = GridSearchCV(svm.LinearSVC(), param_grid, cv=5, n_jobs=-1)\n",
        "   \n",
        "    clf.fit(x_train, y_train)\n",
        "    # print(\"Best parameters set found on development set:\\n\")\n",
        "    # print(clf.best_estimator_)\n",
        "    print(\"\\nGrid scores on development set:\\n\")\n",
        "    test_score = clf.cv_results_['mean_test_score']\n",
        "    test_std = clf.cv_results_['std_test_score']\n",
        "    params = clf.cv_results_['params']\n",
        "    for i in range(len(test_score)):\n",
        "        print(\"%0.3f (+/-%0.03f) for %r\" % (test_score[i], test_std[i] * 2, params[i]))\n",
        "    print(\"\\nDetailed classification report:\\n\")\n",
        "    print(\"The model is trained on the full development set.\")\n",
        "    print(\"The scores are computed on the full evaluation set.\\n\")\n",
        "    y_true, y_pred = y_test, clf.predict(x_test)\n",
        "    #print(classification_report(y_true, y_pred, target_names=get_label()))\n",
        "    print(classification_report(y_true, y_pred))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tHCoGO-zSzi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b4d4f180-5bbe-459a-fca5-007b470b53b9"
      },
      "source": [
        "# Feature(SIFT Descriptors) Extraction\n",
        "\n",
        "if feature_extraction:\n",
        "    # Training\n",
        "    if SIFT_type.lower() =='sparse' or SIFT_type.lower() =='s':\n",
        "        print('Sparse SIFT feature extraction')\n",
        "        x_train = [extract_sift_descriptors(img) for img in x_train]\n",
        "        x_test = [extract_sift_descriptors(img) for img in x_test]\n",
        "        # Remove None in SIFT extraction\n",
        "        x_train = [each for each in zip(x_train, y_train) if not each[0] is None]\n",
        "        x_train, y_train = zip(*x_train)\n",
        "        x_test = [each for each in zip(x_test, y_test) if not each[0] is None]\n",
        "        x_test, y_test = zip(*x_test)\n",
        "    elif SIFT_type.lower() =='dense' or SIFT_type.lower() =='d':\n",
        "        print(\"Dense SIFT feature extraction\")\n",
        "        x_train = [extract_DenseSift_descriptors(img) for img in x_train]\n",
        "        x_test = [extract_DenseSift_descriptors(img) for img in x_test]\n",
        "        x_train_kp, x_train = zip(*x_train)\n",
        "        x_test_kp, x_test = zip(*x_test)\n",
        "        \n",
        "    with open('./x_train.pkl','wb') as f:\n",
        "        pickle.dump(x_train, f)\n",
        "    with open('./x_test.pkl','wb') as f:\n",
        "        pickle.dump(x_test, f)\n",
        "    with open('./y_train.pkl', 'wb') as f:\n",
        "        pickle.dump(y_train, f)\n",
        "    with open('./y_test.pkl', 'wb') as f:\n",
        "        pickle.dump(y_test, f)\n",
        "    \n",
        "      \n",
        "        \n",
        "else:\n",
        "    with open('./x_train.pkl','rb') as fp:\n",
        "        x_train = pickle.load(fp)\n",
        "    with open('./x_test.pkl','rb') as fp:\n",
        "        x_test = pickle.load(fp)\n",
        "    with open('./y_train.pkl','rb') as fp:\n",
        "        y_train = pickle.load(fp)\n",
        "    with open('./y_test.pkl','rb') as fp:\n",
        "        y_test = pickle.load(fp)\n",
        "    print(\"SIFT features ard loaded\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sparse SIFT feature extraction\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsHIZ_cRwsl5"
      },
      "source": [
        "# class_value, class_count = np.unique(y_train, return_counts=True)\n",
        "# print('label value :',class_value)\n",
        "# print('count per label :', class_count)\n",
        "# print('Bootstrap n={} resampling'.format(n))\n",
        "# x_train, y_train = bootstrap_x_y_resample(x_train, y_train, n=n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaXmNIYrzXUS"
      },
      "source": [
        "# Builing Codebook and Clustering\n",
        "codebook =None\n",
        "num_center = None\n",
        "if clustering_and_build_codebook:\n",
        "    print(\"Building the codebook, Wait a minute\")\n",
        "    codebook = build_codebook(x_train, voc_size=VOC_SIZE)\n",
        "    print('codebook shape : ',codebook.shape)\n",
        "    num_center = len(codebook)\n",
        "    with open('./bow_codebook.pkl', 'wb') as f:\n",
        "        pickle.dump(codebook, f)\n",
        "else :\n",
        "    print('Loading the last codebook.')\n",
        "    with open('./bow_codebook.pkl','rb') as fp:\n",
        "        codebook = pickle.load(fp)\n",
        "        num_center = len(codebook)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bn-bCTaJzZXA"
      },
      "source": [
        "# Histogram of the frequency of code words by VQ\n",
        "if image_representation:\n",
        "    print('Approximate data as codeword vectors by Vector Quantization and Making the histogram of data ')\n",
        "    print(\"Bag of words encoding\")\n",
        "    x_train = [input_vector_encoder(x, codebook) for x in x_train]\n",
        "    x_train = np.asarray(x_train)\n",
        "    x_test = [input_vector_encoder(each, codebook) for each in x_test]\n",
        "    x_test = np.asarray(x_test)\n",
        "    with open('./x_train_hist.pkl','wb') as f:\n",
        "        pickle.dump(x_train, f)\n",
        "    with open('./x_test_hist.pkl','wb') as f:\n",
        "        pickle.dump(x_test, f)\n",
        "else :\n",
        "    print('Loading the last Image representation')\n",
        "    with open('./x_train_hist.pkl','rb') as fp:\n",
        "        x_train = pickle.load(fp)\n",
        "    with open('./x_test_hist.pkl','rb') as fp:\n",
        "        x_test = pickle.load(fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJrjkWBtMeQv"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHIzZkmspuBR"
      },
      "source": [
        "y_train= np.array([int(y)for y in y_train])\n",
        "y_test = np.array([int(y)for y in y_test])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTwxCWsJRw2h"
      },
      "source": [
        "## SVM-Gpu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xf-kbLQOdgRG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "0969c711-5c45-4ff5-89bc-524221085072"
      },
      "source": [
        "# To use cupy\n",
        "! curl https://colab.chainer.org/install | sh -"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  1580  100  1580    0     0   8777      0 --:--:-- --:--:-- --:--:--  8777\n",
            "+ apt -y -q install cuda-libraries-dev-10-0\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "cuda-libraries-dev-10-0 is already the newest version (10.0.130-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 4 not upgraded.\n",
            "+ pip install -q cupy-cuda100  chainer \n",
            "+ set +ex\n",
            "Installation succeeded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGnv66iEIe46"
      },
      "source": [
        "# # SVM-Gpu\n",
        "# import warnings\n",
        "# warnings.filterwarnings('ignore')\n",
        "# from sklearn.model_selection import GridSearchCV\n",
        "# from sklearn.svm import SVC\n",
        "# from sklearn import preprocessing\n",
        "# import time\n",
        "# import pdb\n",
        "# def timeit(f):\n",
        "\n",
        "#     def timed(*args, **kw):\n",
        "\n",
        "#         ts = time.time()\n",
        "#         result = f(*args, **kw)\n",
        "#         te = time.time()\n",
        "        \n",
        "#         print('func:%r args:%r took: %2.4f sec' % (f.__name__, kw, te-ts))\n",
        "#         return result\n",
        "\n",
        "#     return timed\n",
        "\n",
        "# @timeit\n",
        "# def train_and_compute_misclassification(kernel, kernel_params, classification_strategy, x_train, y_train, x_test, y_test, lambduh=1, use_optimal_lambda=False, n_folds=3, max_iter=200):   \n",
        "#     print('svm-gpu, {} kernel, parameters {}'.format(kernel, kernel_params))\n",
        "    \n",
        "#     svm = SVM(kernel, kernel_params, lambduh, max_iter, classification_strategy, x=x_train, y=y_train, n_folds=n_folds, display_plots=True)\n",
        "    \n",
        "#     svm.fit(x_train, y_train, use_optimal_lambda=use_optimal_lambda)\n",
        "\n",
        "#     misclassification_error = svm.compute_misclassification_error(x_test, y_test)\n",
        "#     print('Misclassification error (test), {}, {}lambda = {} : {}\\n'.format(svm._classification_strategy, ('optimal ' if use_optimal_lambda else ''), svm._lambduh, misclassification_error))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUrRmWXmoT-i"
      },
      "source": [
        "# import cupy as xp \n",
        "\n",
        "# # Move data to GPU\n",
        "# x_train = xp.asarray(x_train)\n",
        "# x_test =  xp.asarray(x_test)\n",
        "# y_train = xp.asarray(y_train)\n",
        "# y_test = xp.asarray(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpYsiNLwHVt0"
      },
      "source": [
        "# # SVM-Gpu\n",
        "# from sklearn.metrics.pairwise import rbf_kernel\n",
        "# import numpy as np \n",
        "# import scipy.linalg \n",
        "# import pandas as pd \n",
        "# import matplotlib.pyplot as plt\n",
        "# import itertools\n",
        "# import pdb\n",
        "# xp.random.seed(0)\n",
        "# def kernel_linear(x1, x2, params):\n",
        "#     return x1.dot(x2.T)\n",
        "\n",
        "# def kernel_poly(x1, x2, params):\n",
        "#     return (x1.dot(x2.T) + 1)**params['degree']\n",
        "\n",
        "# def kernel_rbf(x1, x2, params):\n",
        "#     if x2.ndim == 2:\n",
        "#         return xp.exp(-xp.linalg.norm(xp.subtract(x1[:, :, xp.newaxis], x2[:, :, xp.newaxis].T), axis=1)**2/params['sigma']**2)\n",
        "#     else:\n",
        "#         return xp.exp(-xp.linalg.norm(xp.subtract(x1, x2), axis=1) ** 2 / params['sigma'] ** 2)\n",
        "\n",
        "# def kernel_rbf_sklearn(x1, x2, params):\n",
        "#     return xp.asarray(rbf_kernel(xp.asnumpy(x1), gamma=params['sigma']))\n",
        "\n",
        "# def kernel_sigmoid(x1, x2, params):\n",
        "#     return xp.tanh(params['alpha'] * (x1.dot(x2.T)) + params['beta'])\n",
        "    \n",
        "# kernel_dict = {'linear': kernel_linear,\n",
        "#                'poly': kernel_poly,\n",
        "#                'rbf': kernel_rbf,\n",
        "#                'rbf_sklearn': kernel_rbf_sklearn,\n",
        "#                'sigmoid': kernel_sigmoid\n",
        "#                 }\n",
        "\n",
        "\n",
        "# class SVM():\n",
        "#     \"\"\"support vector machine\"\"\"\n",
        "\n",
        "#     def __init__(self, kernel, kernel_params, lambduh=1, max_iter=1000, classification_strategy='ovr', x=None, y=None, n_folds=3, lambda_vals=None, use_optimal_lambda=False, display_plots=False, logging=False):\n",
        "#         \"\"\"initialize the classifier\"\"\"\n",
        "\n",
        "#         self._kernel = kernel\n",
        "#         self._kernel_params = kernel_params\n",
        "#         self._lambduh = lambduh\n",
        "#         self._max_iter = max_iter\n",
        "#         self._classification_strategy = classification_strategy\n",
        "#         self._y = y\n",
        "#         self._set_x(x)\n",
        "\n",
        "#         self._coef_matrix = []\n",
        "\n",
        "#         self._n_folds = n_folds\n",
        "#         self._display_plots = display_plots\n",
        "#         self._logging = logging\n",
        "#         self._lambda_vals = lambda_vals\n",
        "#         if self._lambda_vals is None:\n",
        "#             self._lambda_vals = [10**i for i in range(-3, 4)]\n",
        "#         self._use_optimal_lambda = use_optimal_lambda\n",
        "\n",
        "#     def fit(self, x=None, y=None, prevent_relabel=False, use_optimal_lambda=False):\n",
        "#         \"\"\"Trains the kernel support vector machine with the huberized hinge loss\"\"\"\n",
        "#         self._set_x(x)\n",
        "#         if y is not None:\n",
        "#             self._y = y\n",
        "#         self._K = self._compute_gram_matrix()\n",
        "#         self._n = len(self._x)\n",
        "#         objective_val_size = int(self._max_iter//10) + (1 if self._max_iter % 10 == 0 else 0)\n",
        "#         self._objective_val_per_iter = xp.zeros(objective_val_size)\n",
        "        \n",
        "#         if self._classification_strategy == 'ovr':\n",
        "#             iterate_over = xp.asarray(xp.unique(xp.asnumpy(self._y)))\n",
        "            \n",
        "#         elif self._classification_strategy == 'ovo' or self._classification_strategy == 'binary':\n",
        "#             iterate_over = SVM._get_unique_pairs(self._y)\n",
        "\n",
        "#         if self._use_optimal_lambda or use_optimal_lambda:\n",
        "#             self._lambduh, misclassification_error = self.compute_optimal_lambda()\n",
        "#             print('Misclassification error (train), {}, optimal lambda = {} : {}'.format(self._classification_strategy, self._lambduh, misclassification_error))\n",
        "\n",
        "#         for i in range(len(iterate_over)):\n",
        "#             if self._logging:\n",
        "#                 print('Training classifier {} of {}'.format(i + 1, len(iterate_over)))\n",
        "            \n",
        "#             if self._classification_strategy == 'ovr':\n",
        "#                 primary_class = iterate_over[i]\n",
        "#                 x_filtered, y_filtered = SVM._filter_data_by_class_ovr(self._x, self._y, primary_class)\n",
        "#             elif self._classification_strategy == 'ovo':\n",
        "#                 pair = iterate_over[i]\n",
        "#                 x_filtered, y_filtered = SVM.filter_data_by_class_ovo(self._x, self._y, pair, prevent_relabel)\n",
        "#             elif self._classification_strategy == 'binary':\n",
        "#                 pair = iterate_over[i]\n",
        "#                 self._n = len(self._x)\n",
        "#                 self._K = self._compute_gram_matrix()\n",
        "#                 if prevent_relabel:\n",
        "#                     self._primary_class = 1\n",
        "#                     self._secondary_class = -1\n",
        "#                 else:\n",
        "#                     self._primary_class = pair[0]\n",
        "#                     self._secondary_class = pair[1]\n",
        "#                     self._y = xp.where(self._y == self._primary_class, 1, -1)\n",
        "#                 self._coef_matrix, self._objective_val_per_iter, self._misclassification_error_per_iter = self._fast_gradient_descent()\n",
        "#                 return\n",
        "\n",
        "#             svm = SVM(self._kernel, self._kernel_params, self._lambduh, self._max_iter, 'binary', display_plots= True)\n",
        "            \n",
        "#             svm.fit(x_filtered, y_filtered, prevent_relabel=True)\n",
        "            \n",
        "#             self._coef_matrix.append(svm._coef_matrix)\n",
        "#             self._objective_val_per_iter += svm._objective_val_per_iter * (1/len(iterate_over))\n",
        "#         if self._display_plots:\n",
        "#             self.objective_plot()\n",
        "#         return\n",
        "\n",
        "#     def cross_validation_error(self):\n",
        "#         error_per_lambda = xp.zeros(len(self._lambda_vals))\n",
        "\n",
        "#         for i in range(len(self._lambda_vals)):\n",
        "#             lambduh = self._lambda_vals[i]\n",
        "#             if self._logging:\n",
        "#                 print('lambduh = {} ({} of {})'.format(lambduh, i + 1, num_lambda))\n",
        "#             error_per_fold = xp.zeros(self._n_folds)\n",
        "#             for j in range(self._n_folds):\n",
        "#                 fold_size = int(self._n/self._n_folds)\n",
        "#                 indicies = xp.array(range(0, self._n))\n",
        "#                 fold_indicies = ((indicies >= fold_size*j) & (indicies <= fold_size*(j+1)))\n",
        "#                 x_train = self._x[fold_indicies == True]\n",
        "#                 y_train = self._y[fold_indicies == True]       \n",
        "#                 x_test = self._x[fold_indicies == False]\n",
        "#                 y_test = self._y[fold_indicies == False]\n",
        "#                 y_train = y_train.reshape((len(y_train), 1))\n",
        "#                 y_test = y_test.reshape((len(y_test), 1))\n",
        "\n",
        "#                 y_train = xp.ravel(y_train)\n",
        "#                 y_test = xp.ravel(y_test)\n",
        "                \n",
        "#                 svm = SVM(self._kernel, self._kernel_params, lambduh, self._max_iter, self._classification_strategy)\n",
        "#                 svm.fit(x_train, y_train)   \n",
        "#                 error_per_fold[j] = svm.compute_misclassification_error(x_test, y_test)\n",
        "\n",
        "#             error_per_lambda[i] = xp.mean(error_per_fold)\n",
        "            \n",
        "#         return error_per_lambda.tolist()\n",
        "\n",
        "#     def compute_optimal_lambda(self):\n",
        "#         cross_validation_error = self.cross_validation_error()\n",
        "#         if self._display_plots:\n",
        "#             df = pd.DataFrame({'lambda':self._lambda_vals, 'Cross validation error':xp.asnumpy(cross_validation_error)})\n",
        "#             display(df)\n",
        "#             df.plot('lambda', 'Cross validation error', logx=True)\n",
        "#             plt.show()\n",
        "#         return self._lambda_vals[np.nanargmin(cross_validation_error)], np.min(cross_validation_error)\n",
        "\n",
        "#     def compute_misclassification_error(self, x, y): \n",
        "#         y_pred = self.predict(x)\n",
        "#         return xp.mean(y_pred != y)\n",
        "\n",
        "#     def predict(self, x):\n",
        "#         x = self._standardize(x)\n",
        "#         if self._classification_strategy == 'ovr':\n",
        "#             return self._predict_ovr(x)\n",
        "#         elif self._classification_strategy == 'ovo':\n",
        "#             return self._predict_ovo(x)\n",
        "#         else:\n",
        "#             return self._predict_binary(x)\n",
        "\n",
        "#     def objective_plot(self):\n",
        "#         fig, ax = plt.subplots()\n",
        "#         ax.plot(np.array(range(len(self._objective_val_per_iter)))*10, xp.asnumpy(self._objective_val_per_iter), label='Train', c='red')\n",
        "#         plt.xlabel('Iteration')\n",
        "#         plt.ylabel('Objective value')\n",
        "#         plt.title('Objective value vs. iteration when lambda=' + str(self._lambduh))\n",
        "#         ax.legend(loc='upper right') \n",
        "#         plt.show()\n",
        "\n",
        "#     def plot_misclassification_error(self): \n",
        "#         if self._classification_strategy == 'binary':\n",
        "#             fig, ax = plt.subplots() \n",
        "#             ax.plot(np.array(range(len(self._misclassification_error_per_iter)))*10, xp.asnumpy(self._misclassification_error_per_iter), label='Train', c='red') \n",
        "#             plt.xlabel('Iteration') \n",
        "#             plt.ylabel('Misclassification error') \n",
        "#             plt.title('Misclassification error vs iteration') \n",
        "#             ax.legend(loc='upper right') \n",
        "#             plt.show()\n",
        "#         else:\n",
        "#             print('Plotting misclassification error only available for binary classification.')\n",
        "\n",
        "#     def _set_x(self, x):\n",
        "#         if x is not None:\n",
        "#             self._n = len(x)\n",
        "#             self._x_sd = xp.std(x, axis=0)\n",
        "#             self._x_mean = xp.mean(x, axis=0)\n",
        "#             self._x = self._standardize(x)\n",
        "\n",
        "#     def _standardize(self, x):\n",
        "#         sd = self._x_sd\n",
        "#         mean = self._x_mean\n",
        "#         mean[sd == 0] = 0\n",
        "#         sd[sd == 0] = 1\n",
        "#         return (x - mean) / sd\n",
        "\n",
        "#     @staticmethod\n",
        "#     def filter_data_by_class_ovo(x, y, classes, prevent_relabel=False):\n",
        "#         x = SVM._select_classes(x, y, classes)\n",
        "#         y = SVM._select_classes(y, y, classes)\n",
        "#         if prevent_relabel == False:\n",
        "#             y = xp.where(y == classes[0], 1, -1)\n",
        "                    \n",
        "#         return x, y\n",
        "\n",
        "#     @staticmethod\n",
        "#     def subset_data(x, y, max_samples):\n",
        "#         if max_samples is None or max_samples > len(x):\n",
        "#             return x, y\n",
        "#         else:\n",
        "#             idx = np.random.choice(np.arange(len(x)), max_samples, replace=False)\n",
        "#             return x[idx], y[idx]\n",
        "#     @staticmethod   \n",
        "#     def subset_data_gpu(x, y, max_samples):\n",
        "#         x, y = subset_data(x, y, max_samples)\n",
        "#         return xp.asarray(x), xp.asarray(y)\n",
        "\n",
        "#     @staticmethod\n",
        "#     def _get_unique_pairs(y):\n",
        "#         return pd.Series(list(itertools.combinations(np.unique(xp.asnumpy(y)),2)))\n",
        "\n",
        "#     @staticmethod\n",
        "#     def _select_classes(x, y, classes):\n",
        "#         if len(classes) == 2:\n",
        "#             return x[(y == classes[0]) | (y == classes[1])]\n",
        "#         else:\n",
        "#             return x[xp.asarray(np.isin(xp.asnumpy(y), classes))]\n",
        "\n",
        "#     @staticmethod\n",
        "#     def _select_classes_ovr(x, y, primary_class):\n",
        "#         positive = x[y == primary_class]\n",
        "#         negative = x[y != primary_class]\n",
        "#         # Get random rows of the same length as the positive matrix\n",
        "#         if len(positive) < len(negative):  \n",
        "#             negative = negative[xp.random.choice(negative.shape[0], len(positive), replace=False)]\n",
        "#         return xp.concatenate((positive, negative), axis=0)\n",
        "\n",
        "#     @staticmethod\n",
        "#     def _filter_data_by_class_ovr(x, y, primary_class):\n",
        "#         x = SVM._select_classes_ovr(x, y, primary_class)\n",
        "#         y = SVM._select_classes_ovr(y, y, primary_class)\n",
        "#         y = xp.where(y == primary_class, 1, -1)\n",
        "        \n",
        "#         return x, y\n",
        "\n",
        "#     def _compute_gradient(self, alpha):\n",
        "#         \"\"\"Computes the gradient ∇F(β) of F\"\"\"\n",
        "#         K_alpha = xp.dot(self._K, alpha)\n",
        "#         grad = -2 / self._n * xp.sum(self._y[:, xp.newaxis] * self._K * xp.max(xp.stack((xp.zeros_like(self._y), 1 - self._y * K_alpha)), axis=0)[:, xp.newaxis], axis=0) + 2 * self._lambduh * K_alpha    \n",
        "#         return grad\n",
        "\n",
        "#     def _objective(self, alpha):\n",
        "#         K_alpha = xp.dot(self._K, alpha)\n",
        "#         return 1 / self._n * xp.sum(xp.max(xp.stack((xp.zeros_like(self._y), 1 - self._y * K_alpha)), axis=0) ** 2) + self._lambduh * alpha.dot(K_alpha)\n",
        "\n",
        "#     def _backtracking_line_search(self, alpha, eta=1, alphaparam=0.5, betaparam=0.8, max_iter=100): \n",
        "#         grad_alpha = self._compute_gradient(alpha) \n",
        "#         norm_grad_alpha = xp.linalg.norm(grad_alpha) \n",
        "#         found_eta = 0 \n",
        "#         iter = 0 \n",
        "#         while found_eta == 0 and iter < max_iter: \n",
        "#             if self._objective(alpha - eta * grad_alpha) < self._objective(alpha) - alphaparam * eta * norm_grad_alpha ** 2: \n",
        "#                 found_eta = 1 \n",
        "#             elif iter == max_iter: \n",
        "#                 raise ('Max number of iterations of backtracking line search reached') \n",
        "#             else: \n",
        "#                 eta *= betaparam \n",
        "#                 iter += 1 \n",
        "#             return eta\n",
        "\n",
        "#     def _compute_gram_matrix(self):\n",
        "#         \"\"\"Computes, for any set of datapoints x1,...,xn, the kernel matrix K\"\"\"\n",
        "#         kernel = self._kernel\n",
        "#         if kernel == 'rbf':\n",
        "#             kernel += '_sklearn'\n",
        "#         gram = kernel_dict[kernel](self._x, self._x, self._kernel_params)\n",
        "#         return gram\n",
        "\n",
        "#     # \n",
        "#     def _kernel_eval(self, x, x_train):\n",
        "#         keval = kernel_dict[self._kernel](x_train, x, self._kernel_params)\n",
        "#         return keval\n",
        "\n",
        "#     def _fast_gradient_descent(self):\n",
        "#         eta_init = self._optimal_eta_init()\n",
        "\n",
        "#         alpha = xp.zeros(self._n)\n",
        "#         theta = xp.zeros(self._n)\n",
        "#         eta = eta_init\n",
        "#         grad_theta = self._compute_gradient(theta)\n",
        "#         objective_val_size = int(self._max_iter//10) + (1 if self._max_iter % 10 == 0 else 0)\n",
        "#         objective_vals = xp.ones(objective_val_size)\n",
        "#         misclassification_error_per_iter = xp.ones(objective_val_size)\n",
        "#         iter = 0\n",
        "#         while iter < self._max_iter:\n",
        "#             eta = self._backtracking_line_search(theta, eta=eta)\n",
        "#             alpha_new = theta - eta * grad_theta\n",
        "#             theta = alpha_new + iter / (iter + 3) * (alpha_new - alpha)\n",
        "#             grad_theta = self._compute_gradient(theta)\n",
        "#             alpha = alpha_new\n",
        "#             iter += 1\n",
        "#             if self._display_plots and iter % 10 == 0:\n",
        "#                 objective_vals[int(iter/10)] = self._objective(alpha)\n",
        "#                 self._coef_matrix = alpha\n",
        "#                 misclassification_error_per_iter[int(iter/10)] = self.compute_misclassification_error(self._x, xp.where(self._y > 0, self._primary_class, self._secondary_class))\n",
        "            \n",
        "#         return alpha, objective_vals, misclassification_error_per_iter\n",
        "\n",
        "#     def _predict_binary(self, x):      \n",
        "#         return self._prediction_binary(self._coef_matrix, x, self._x, self._primary_class, self._secondary_class)\n",
        "\n",
        "#     def _predict_ovo(self, x):        \n",
        "#         def mode(a):\n",
        "#             counts = xp.bincount(a.astype(xp.int64))\n",
        "#             return xp.argmax(counts)\n",
        "#         pairs = SVM._get_unique_pairs(self._y)\n",
        "#         predictions = xp.zeros((x.shape[0], len(pairs)))\n",
        "#         for i in range(len(pairs)):\n",
        "#             pair = pairs[i]\n",
        "#             alpha = self._coef_matrix[i]\n",
        "#             x_train_filtered, y_train_filtered = SVM.filter_data_by_class_ovo(self._x, self._y, pair)\n",
        "#             y_pred = self._prediction_binary(alpha, x, x_train_filtered, pair[0], pair[1])\n",
        "#             predictions[:,i] = y_pred\n",
        "#         return xp.stack([mode(p) for p in predictions])\n",
        "\n",
        "#     def _predict_ovr(self, x):\n",
        "#         y_unique = xp.asarray(xp.unique(xp.asnumpy(self._y)))\n",
        "#         prediction_probabilities = xp.zeros((x.shape[0], len(y_unique)))\n",
        "#         for i in range(len(y_unique)):\n",
        "#             alpha = self._coef_matrix[i]\n",
        "#             x_train_filtered, y_train_filtered = SVM._filter_data_by_class_ovr(self._x, self._y, y_unique[i])\n",
        "#             pred_prob = self._prediction_prob(alpha, x, x_train_filtered)\n",
        "#             prediction_probabilities[:,i] = pred_prob \n",
        "\n",
        "#         return xp.stack([y_unique[xp.argmax(p)] for p in prediction_probabilities])\n",
        "\n",
        "#     def _prediction_prob(self, alpha, x, x_train):\n",
        "#         return xp.dot(self._kernel_eval(x, x_train).T, alpha)\n",
        "\n",
        "#     def _prediction_binary(self, alpha, x, x_train, class1=1, class2=-1):\n",
        "#         pred_prob = self._prediction_prob(alpha, x, x_train)\n",
        "#         return xp.where(pred_prob > 0, class1, class2)\n",
        "    \n",
        "#     def _optimal_eta_init(self):\n",
        "#         return 1 / scipy.linalg.eigh(xp.asnumpy(2 / self._n * xp.dot(self._K, self._K) + 2 * self._lambduh * self._K), eigvals=(self._n - 1, self._n - 1), eigvals_only=True)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VE7nH2W_ILg2"
      },
      "source": [
        "# train_and_compute_misclassification('linear',{}, 'ovr', x_train, y_train, x_test, y_test,lambduh=0.01, use_optimal_lambda=False)\n",
        "# train_and_compute_misclassification('linear',{}, 'ovr', x_train, y_train, x_test, y_test,lambduh=0.1, use_optimal_lambda=False)\n",
        "# train_and_compute_misclassification('linear',{}, 'ovr', x_train, y_train, x_test, y_test,lambduh=1, use_optimal_lambda=False)\n",
        "# train_and_compute_misclassification('linear',{}, 'ovr', x_train, y_train, x_test, y_test,lambduh=10, use_optimal_lambda=False)\n",
        "# train_and_compute_misclassification('linear',{}, 'ovr', x_train, y_train, x_test, y_test,lambduh=100, use_optimal_lambda=False)\n",
        "# train_and_compute_misclassification('linear',{}, 'ovr', x_train, y_train, x_test, y_test,lambduh=1000, use_optimal_lambda=False)\n",
        "# train_and_compute_misclassification('linear',{}, 'ovr', x_train, y_train, x_test, y_test,lambduh=10000, use_optimal_lambda=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irPFciBQzcNV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "54932c1d-3fd0-4699-aa7d-36e19e0174c7"
      },
      "source": [
        "if learning_and_recognition:\n",
        "    print('Dataset : caltech 101')\n",
        "    print('SIFT Type :',SIFT_type.upper())\n",
        "    print('Train Size : {}, Test Size : {}'.format(len(y_train), len(y_test)))\n",
        "    print('k-means clustering-K :',num_center)\n",
        "    svm_classifier(x_train, y_train, x_test,y_test,PARAM_GRID)\n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset : caltech 101\n",
            "SIFT Type : SPARSE\n",
            "Train Size : 3060, Test Size : 6084\n",
            "k-means clustering-K : 200\n",
            "Tuning hyper-parameters\n",
            "\n",
            "\n",
            "Grid scores on development set:\n",
            "\n",
            "1.000 (+/-0.000) for {'C': 1.0}\n",
            "1.000 (+/-0.000) for {'C': 10.0}\n",
            "\n",
            "Detailed classification report:\n",
            "\n",
            "The model is trained on the full development set.\n",
            "The scores are computed on the full evaluation set.\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        25\n",
            "           1       0.50      0.00      0.00       770\n",
            "           2       0.00      0.00      0.00        12\n",
            "           3       0.00      0.00      0.00        12\n",
            "           4       0.18      0.01      0.03       437\n",
            "           5       0.00      0.00      0.00        17\n",
            "           6       0.00      0.00      0.00        24\n",
            "           7       0.00      0.00      0.00        16\n",
            "           8       0.00      0.00      0.00         3\n",
            "           9       0.00      0.00      0.00        98\n",
            "          10       0.00      0.00      0.00        68\n",
            "          11       0.00      0.00      0.00        13\n",
            "          12       0.00      0.00      0.00        55\n",
            "          13       0.00      0.00      0.00        61\n",
            "          14       0.00      0.00      0.00        20\n",
            "          15       0.00      0.00      0.00        13\n",
            "          16       0.00      0.00      0.00        93\n",
            "          17       0.00      0.00      0.00        17\n",
            "          18       0.00      0.00      0.00        29\n",
            "          19       0.00      0.00      0.00        32\n",
            "          20       0.00      0.00      0.00        77\n",
            "          21       0.00      0.00      0.00        17\n",
            "          22       0.00      0.00      0.00        39\n",
            "          23       0.00      0.00      0.00        43\n",
            "          24       0.00      0.00      0.00        40\n",
            "          25       0.00      0.00      0.00        20\n",
            "          26       0.00      0.00      0.00        21\n",
            "          27       0.00      0.00      0.00        27\n",
            "          28       0.00      0.00      0.00        37\n",
            "          29       0.00      0.00      0.00        22\n",
            "          30       0.00      0.00      0.00        35\n",
            "          31       0.00      0.00      0.00        38\n",
            "          32       0.01      0.02      0.01        45\n",
            "          33       0.00      0.00      0.00        34\n",
            "          34       0.00      0.00      0.00        23\n",
            "          35       0.01      0.03      0.01        34\n",
            "          36       0.00      0.00      0.00        55\n",
            "          37       0.00      0.00      0.00       405\n",
            "          38       0.16      0.03      0.05       405\n",
            "          39       0.00      0.00      0.00        37\n",
            "          40       0.00      0.00      0.00        37\n",
            "          41       0.00      0.00      0.00        15\n",
            "          42       0.00      0.00      0.00         4\n",
            "          43       0.00      0.00      0.00         4\n",
            "          44       0.00      0.00      0.00        21\n",
            "          45       0.02      0.03      0.02        69\n",
            "          46       0.00      0.00      0.00        70\n",
            "          47       0.00      0.00      0.00        12\n",
            "          48       0.00      0.00      0.00        24\n",
            "          49       0.00      0.00      0.00        58\n",
            "          50       0.00      0.00      0.00        50\n",
            "          51       0.00      0.00      0.00         1\n",
            "          52       0.00      0.00      0.00        34\n",
            "          53       0.00      0.00      0.00        56\n",
            "          54       0.06      0.04      0.04        84\n",
            "          55       0.00      0.00      0.00        31\n",
            "          56       0.00      0.00      0.00        51\n",
            "          57       0.00      0.00      0.00       170\n",
            "          58       0.00      0.00      0.00        48\n",
            "          59       0.00      0.00      0.00        11\n",
            "          60       0.00      0.00      0.00        36\n",
            "          61       0.00      0.00      0.00        13\n",
            "          62       0.03      0.10      0.04        10\n",
            "          63       0.00      0.00      0.00        57\n",
            "          64       0.00      0.00      0.00         2\n",
            "          65       0.00      0.00      0.00        46\n",
            "          66       0.02      0.00      0.00       768\n",
            "          67       0.04      0.04      0.04        25\n",
            "          68       0.01      0.20      0.02         5\n",
            "          69       0.00      0.00      0.00         9\n",
            "          70       0.01      0.06      0.02        17\n",
            "          71       0.00      0.00      0.00         8\n",
            "          72       0.00      0.00      0.00        15\n",
            "          73       0.00      0.00      0.00        23\n",
            "          74       0.00      0.00      0.00         4\n",
            "          75       0.04      0.04      0.04        27\n",
            "          76       0.02      0.02      0.02        52\n",
            "          77       0.00      0.00      0.00        29\n",
            "          78       0.00      0.11      0.01        19\n",
            "          79       0.00      0.00      0.00        10\n",
            "          80       0.00      0.00      0.00        33\n",
            "          81       0.00      0.00      0.00         9\n",
            "          82       0.00      0.00      0.00        54\n",
            "          83       0.02      0.07      0.03        27\n",
            "          84       0.00      0.00      0.00         5\n",
            "          85       0.00      0.00      0.00        34\n",
            "          86       0.00      0.00      0.00        15\n",
            "          87       0.00      0.00      0.00        56\n",
            "          88       0.00      0.00      0.00        29\n",
            "          89       0.00      0.00      0.00        34\n",
            "          90       0.00      0.00      0.00         5\n",
            "          91       0.00      0.00      0.00        55\n",
            "          92       0.00      0.05      0.00        19\n",
            "          93       0.00      0.00      0.00        56\n",
            "          94       0.00      0.00      0.00        45\n",
            "          95       0.01      0.00      0.01       209\n",
            "          96       0.00      0.00      0.00         7\n",
            "          97       0.00      0.00      0.00        29\n",
            "          98       0.01      0.25      0.01         4\n",
            "          99       0.00      0.00      0.00        26\n",
            "         100       0.00      0.00      0.00         9\n",
            "         101       0.00      0.00      0.00        30\n",
            "\n",
            "    accuracy                           0.01      6084\n",
            "   macro avg       0.01      0.01      0.00      6084\n",
            "weighted avg       0.09      0.01      0.01      6084\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX1mSqgHr51l"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}